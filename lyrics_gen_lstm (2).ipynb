{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lyrics_gen_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKndmYWYc-Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  !pip install keras_efficientnets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XUM3pOAdDKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/tensorflow/serving.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scffk4vY41hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cd serving"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ0pqEh55GAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlG3Fjo65HMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxwBUPjA98YA",
        "colab_type": "code",
        "outputId": "a0749ee6-1cb2-49fa-e1db-08df3d3db7db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxGSdvl299kg",
        "colab_type": "code",
        "outputId": "9ee3fd78-665d-4e79-b439-6b75d5eed520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My Drive/datasets/logic_lyrics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/datasets/logic_lyrics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSGONtQ2-kBc",
        "colab_type": "code",
        "outputId": "d137b29a-6d00-4986-9db4-2678090785bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logic_lyrics.txt  weights-improvement-01-2.7537.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EHrIHFJ-r1-",
        "colab_type": "code",
        "outputId": "a118581b-9706-45a0-e667-1def7a3e7e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkrAx7O_-05B",
        "colab_type": "code",
        "outputId": "c7d4b8c4-b762-4e6e-90dc-229ac25ad90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_gHGpjh-43l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"logic_lyrics.txt\",\"r\") as f:\n",
        "    text_data = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfkG2YSX_gl4",
        "colab_type": "code",
        "outputId": "320c1d47-9b05-4631-e28a-07bdfed92c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "317323"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X4m1Vz0_hzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data = text_data.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCCnP4h4C_E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re5sHcgvDBkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replace_dict = {\"(\":\"\", \")\":\"\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tXHkRS6B5Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data = multiple_replace(replace_dict, text_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiU39BXUAI8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(text_data)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvd_UQKZHyya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_chars = len(text_data)\n",
        "n_vocab = len(char_to_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laMRNen1BPXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words = text_data.split()\n",
        "# words_to_int = dict((c,i) for i,c in enumerate(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYkM1Eg1ILAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_words = len(words)\n",
        "# n_vocab_words = len(words_to_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIsfbIx0BTnv",
        "colab_type": "code",
        "outputId": "2df98a3f-d82b-4f1f-c2b7-dfcc632133b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length_char = 100\n",
        "dataX_char = []\n",
        "dataY_char = []\n",
        "for i in range(0, n_chars - seq_length_char, 1):\n",
        "\tseq_in_char = text_data[i:i + seq_length_char]\n",
        "\tseq_out_char = text_data[i + seq_length_char]\n",
        "\tdataX_char.append([char_to_int[char] for char in seq_in_char])\n",
        "\tdataY_char.append(char_to_int[seq_out_char])\n",
        "n_patterns_char = len(dataX_char)\n",
        "print (\"Total Patterns: \", n_patterns_char)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  316412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utvc3rq7BjuU",
        "colab_type": "code",
        "outputId": "825dde85-a228-43ee-8f20-fcfa845149c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# # prepare the dataset of input to output pairs encoded as integers\n",
        "# seq_length_words = 10\n",
        "# dataX_words = []\n",
        "# dataY_words = []\n",
        "# for i in range(0, n_words - seq_length_words, 1):\n",
        "# \tseq_in_words = words[i:i + seq_length_words]\n",
        "# \tseq_out_words = words[i + seq_length_words]\n",
        "# \tdataX_words.append([words_to_int[word] for word in seq_in_words])\n",
        "# \tdataY_words.append(words_to_int[seq_out_words])\n",
        "# n_patterns_words = len(dataX_words)\n",
        "# print (\"Total Patterns: \", n_patterns_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  62877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOTkrYGvIjwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X_char = numpy.reshape(dataX_char, (n_patterns_char, seq_length_char, 1))\n",
        "# normalize\n",
        "X_char = X_char / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y_char = np_utils.to_categorical(dataY_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35-8AqzCP7v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQBVcZNJqaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # reshape X to be [samples, time steps, features]\n",
        "# X_words = numpy.reshape(dataX_words, (n_patterns_words, seq_length_words, 1))\n",
        "# # normalize\n",
        "# X_words = X_words / float(n_vocab_words)\n",
        "# # one hot encode the output variable\n",
        "# y_words = np_utils.to_categorical(dataY_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng6AS2KiM-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpaoW3GKEWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyCFh1RjNNLH",
        "colab_type": "code",
        "outputId": "43324286-73a0-4298-f3e6-a12b96920173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X_char.shape[1], X_char.shape[2])))#, return_sequences=True))\n",
        "# model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y_char.shape[1], activation='relu'))\n",
        "model.add(Dense(y_char.shape[1], activation='softmax'))\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 256)               264192    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 51)                13107     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 51)                2652      \n",
            "=================================================================\n",
            "Total params: 279,951\n",
            "Trainable params: 279,951\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSkOC_5SNuJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRBcHlQxQCaf",
        "colab_type": "code",
        "outputId": "66c87602-e055-4220-badc-e53d4aedc48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_char, y_char, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "316412/316412 [==============================] - 240s 757us/step - loss: 2.6115\n",
            "\n",
            "Epoch 00001: loss improved from 2.75506 to 2.61148, saving model to weights-improvement-01-2.6115.hdf5\n",
            "Epoch 2/100\n",
            "316412/316412 [==============================] - 238s 753us/step - loss: 2.5391\n",
            "\n",
            "Epoch 00002: loss improved from 2.61148 to 2.53908, saving model to weights-improvement-02-2.5391.hdf5\n",
            "Epoch 3/100\n",
            "316412/316412 [==============================] - 241s 762us/step - loss: 2.4740\n",
            "\n",
            "Epoch 00003: loss improved from 2.53908 to 2.47402, saving model to weights-improvement-03-2.4740.hdf5\n",
            "Epoch 4/100\n",
            "316412/316412 [==============================] - 239s 755us/step - loss: 2.4183\n",
            "\n",
            "Epoch 00004: loss improved from 2.47402 to 2.41829, saving model to weights-improvement-04-2.4183.hdf5\n",
            "Epoch 5/100\n",
            "316412/316412 [==============================] - 239s 757us/step - loss: 2.3681\n",
            "\n",
            "Epoch 00005: loss improved from 2.41829 to 2.36811, saving model to weights-improvement-05-2.3681.hdf5\n",
            "Epoch 6/100\n",
            "316412/316412 [==============================] - 238s 753us/step - loss: 2.3232\n",
            "\n",
            "Epoch 00006: loss improved from 2.36811 to 2.32319, saving model to weights-improvement-06-2.3232.hdf5\n",
            "Epoch 7/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 2.2848\n",
            "\n",
            "Epoch 00007: loss improved from 2.32319 to 2.28482, saving model to weights-improvement-07-2.2848.hdf5\n",
            "Epoch 8/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 2.2499\n",
            "\n",
            "Epoch 00008: loss improved from 2.28482 to 2.24988, saving model to weights-improvement-08-2.2499.hdf5\n",
            "Epoch 9/100\n",
            "316412/316412 [==============================] - 236s 746us/step - loss: 2.2176\n",
            "\n",
            "Epoch 00009: loss improved from 2.24988 to 2.21764, saving model to weights-improvement-09-2.2176.hdf5\n",
            "Epoch 10/100\n",
            "316412/316412 [==============================] - 236s 747us/step - loss: 2.1882\n",
            "\n",
            "Epoch 00010: loss improved from 2.21764 to 2.18820, saving model to weights-improvement-10-2.1882.hdf5\n",
            "Epoch 11/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 2.1599\n",
            "\n",
            "Epoch 00011: loss improved from 2.18820 to 2.15994, saving model to weights-improvement-11-2.1599.hdf5\n",
            "Epoch 12/100\n",
            "316412/316412 [==============================] - 239s 755us/step - loss: 2.1348\n",
            "\n",
            "Epoch 00012: loss improved from 2.15994 to 2.13476, saving model to weights-improvement-12-2.1348.hdf5\n",
            "Epoch 13/100\n",
            "316412/316412 [==============================] - 237s 750us/step - loss: 2.1125\n",
            "\n",
            "Epoch 00013: loss improved from 2.13476 to 2.11249, saving model to weights-improvement-13-2.1125.hdf5\n",
            "Epoch 14/100\n",
            "316412/316412 [==============================] - 239s 755us/step - loss: 2.0871\n",
            "\n",
            "Epoch 00014: loss improved from 2.11249 to 2.08713, saving model to weights-improvement-14-2.0871.hdf5\n",
            "Epoch 15/100\n",
            "316412/316412 [==============================] - 238s 751us/step - loss: 2.0674\n",
            "\n",
            "Epoch 00015: loss improved from 2.08713 to 2.06736, saving model to weights-improvement-15-2.0674.hdf5\n",
            "Epoch 16/100\n",
            "316412/316412 [==============================] - 237s 750us/step - loss: 2.0472\n",
            "\n",
            "Epoch 00016: loss improved from 2.06736 to 2.04721, saving model to weights-improvement-16-2.0472.hdf5\n",
            "Epoch 17/100\n",
            "316412/316412 [==============================] - 236s 746us/step - loss: 2.0281\n",
            "\n",
            "Epoch 00017: loss improved from 2.04721 to 2.02806, saving model to weights-improvement-17-2.0281.hdf5\n",
            "Epoch 18/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 2.0123\n",
            "\n",
            "Epoch 00018: loss improved from 2.02806 to 2.01229, saving model to weights-improvement-18-2.0123.hdf5\n",
            "Epoch 19/100\n",
            "316412/316412 [==============================] - 237s 750us/step - loss: 1.9948\n",
            "\n",
            "Epoch 00019: loss improved from 2.01229 to 1.99480, saving model to weights-improvement-19-1.9948.hdf5\n",
            "Epoch 20/100\n",
            "316412/316412 [==============================] - 239s 756us/step - loss: 1.9789\n",
            "\n",
            "Epoch 00020: loss improved from 1.99480 to 1.97888, saving model to weights-improvement-20-1.9789.hdf5\n",
            "Epoch 21/100\n",
            "316412/316412 [==============================] - 238s 752us/step - loss: 1.9644\n",
            "\n",
            "Epoch 00021: loss improved from 1.97888 to 1.96438, saving model to weights-improvement-21-1.9644.hdf5\n",
            "Epoch 22/100\n",
            "316412/316412 [==============================] - 237s 750us/step - loss: 1.9537\n",
            "\n",
            "Epoch 00022: loss improved from 1.96438 to 1.95373, saving model to weights-improvement-22-1.9537.hdf5\n",
            "Epoch 23/100\n",
            "316412/316412 [==============================] - 238s 752us/step - loss: 1.9452\n",
            "\n",
            "Epoch 00023: loss improved from 1.95373 to 1.94521, saving model to weights-improvement-23-1.9452.hdf5\n",
            "Epoch 24/100\n",
            "316412/316412 [==============================] - 235s 744us/step - loss: 1.9330\n",
            "\n",
            "Epoch 00024: loss improved from 1.94521 to 1.93301, saving model to weights-improvement-24-1.9330.hdf5\n",
            "Epoch 25/100\n",
            "316412/316412 [==============================] - 236s 746us/step - loss: 1.9190\n",
            "\n",
            "Epoch 00025: loss improved from 1.93301 to 1.91903, saving model to weights-improvement-25-1.9190.hdf5\n",
            "Epoch 26/100\n",
            "316412/316412 [==============================] - 236s 745us/step - loss: 1.9107\n",
            "\n",
            "Epoch 00026: loss improved from 1.91903 to 1.91073, saving model to weights-improvement-26-1.9107.hdf5\n",
            "Epoch 27/100\n",
            "316412/316412 [==============================] - 235s 744us/step - loss: 1.8999\n",
            "\n",
            "Epoch 00027: loss improved from 1.91073 to 1.89989, saving model to weights-improvement-27-1.8999.hdf5\n",
            "Epoch 28/100\n",
            "316412/316412 [==============================] - 237s 748us/step - loss: 1.8901\n",
            "\n",
            "Epoch 00028: loss improved from 1.89989 to 1.89007, saving model to weights-improvement-28-1.8901.hdf5\n",
            "Epoch 29/100\n",
            "316412/316412 [==============================] - 236s 745us/step - loss: 1.8829\n",
            "\n",
            "Epoch 00029: loss improved from 1.89007 to 1.88287, saving model to weights-improvement-29-1.8829.hdf5\n",
            "Epoch 30/100\n",
            "316412/316412 [==============================] - 235s 743us/step - loss: 1.8741\n",
            "\n",
            "Epoch 00030: loss improved from 1.88287 to 1.87412, saving model to weights-improvement-30-1.8741.hdf5\n",
            "Epoch 31/100\n",
            "316412/316412 [==============================] - 235s 744us/step - loss: 1.8673\n",
            "\n",
            "Epoch 00031: loss improved from 1.87412 to 1.86733, saving model to weights-improvement-31-1.8673.hdf5\n",
            "Epoch 32/100\n",
            "316412/316412 [==============================] - 237s 748us/step - loss: 1.8940\n",
            "\n",
            "Epoch 00032: loss did not improve from 1.86733\n",
            "Epoch 33/100\n",
            "316412/316412 [==============================] - 236s 746us/step - loss: 1.8663\n",
            "\n",
            "Epoch 00033: loss improved from 1.86733 to 1.86633, saving model to weights-improvement-33-1.8663.hdf5\n",
            "Epoch 34/100\n",
            "316412/316412 [==============================] - 236s 746us/step - loss: 1.8565\n",
            "\n",
            "Epoch 00034: loss improved from 1.86633 to 1.85654, saving model to weights-improvement-34-1.8565.hdf5\n",
            "Epoch 35/100\n",
            "316412/316412 [==============================] - 238s 751us/step - loss: 1.8563\n",
            "\n",
            "Epoch 00035: loss improved from 1.85654 to 1.85627, saving model to weights-improvement-35-1.8563.hdf5\n",
            "Epoch 36/100\n",
            "316412/316412 [==============================] - 237s 750us/step - loss: 1.8409\n",
            "\n",
            "Epoch 00036: loss improved from 1.85627 to 1.84089, saving model to weights-improvement-36-1.8409.hdf5\n",
            "Epoch 37/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 1.8290\n",
            "\n",
            "Epoch 00037: loss improved from 1.84089 to 1.82896, saving model to weights-improvement-37-1.8290.hdf5\n",
            "Epoch 38/100\n",
            "316412/316412 [==============================] - 239s 754us/step - loss: 1.8516\n",
            "\n",
            "Epoch 00038: loss did not improve from 1.82896\n",
            "Epoch 39/100\n",
            "316412/316412 [==============================] - 240s 759us/step - loss: 1.8171\n",
            "\n",
            "Epoch 00039: loss improved from 1.82896 to 1.81708, saving model to weights-improvement-39-1.8171.hdf5\n",
            "Epoch 40/100\n",
            "316412/316412 [==============================] - 241s 763us/step - loss: 1.8156\n",
            "\n",
            "Epoch 00040: loss improved from 1.81708 to 1.81565, saving model to weights-improvement-40-1.8156.hdf5\n",
            "Epoch 41/100\n",
            "316412/316412 [==============================] - 240s 760us/step - loss: 1.8210\n",
            "\n",
            "Epoch 00041: loss did not improve from 1.81565\n",
            "Epoch 42/100\n",
            "316412/316412 [==============================] - 240s 758us/step - loss: 1.8066\n",
            "\n",
            "Epoch 00042: loss improved from 1.81565 to 1.80661, saving model to weights-improvement-42-1.8066.hdf5\n",
            "Epoch 43/100\n",
            "316412/316412 [==============================] - 241s 761us/step - loss: 1.8012\n",
            "\n",
            "Epoch 00043: loss improved from 1.80661 to 1.80121, saving model to weights-improvement-43-1.8012.hdf5\n",
            "Epoch 44/100\n",
            "316412/316412 [==============================] - 241s 762us/step - loss: 1.8094\n",
            "\n",
            "Epoch 00044: loss did not improve from 1.80121\n",
            "Epoch 45/100\n",
            "316412/316412 [==============================] - 237s 749us/step - loss: 1.8036\n",
            "\n",
            "Epoch 00045: loss did not improve from 1.80121\n",
            "Epoch 46/100\n",
            "316412/316412 [==============================] - 238s 752us/step - loss: 1.7977\n",
            "\n",
            "Epoch 00046: loss improved from 1.80121 to 1.79770, saving model to weights-improvement-46-1.7977.hdf5\n",
            "Epoch 47/100\n",
            "316412/316412 [==============================] - 235s 743us/step - loss: 1.7891\n",
            "\n",
            "Epoch 00047: loss improved from 1.79770 to 1.78912, saving model to weights-improvement-47-1.7891.hdf5\n",
            "Epoch 48/100\n",
            "316412/316412 [==============================] - 239s 757us/step - loss: 1.7791\n",
            "\n",
            "Epoch 00048: loss improved from 1.78912 to 1.77908, saving model to weights-improvement-48-1.7791.hdf5\n",
            "Epoch 49/100\n",
            "255488/316412 [=======================>......] - ETA: 45s - loss: 1.7803Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1kuJaDQSI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"weights-improvement-51-1.7654.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMZ0-xHQW2Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_xdqjP8WpTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "009d9a0c-554a-4488-f485-1d55cbbf7f22"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX_char)-1)\n",
        "pattern = dataX_char[start]\n",
        "#\tprint \"Seed:\"\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" ey would never hate it, i'm a renegade it\n",
            "say they love me, say they want me\n",
            "say they bump me in the \"\n",
            " soued if to be and get tp and ree and she sacond i got the seil to be alack i'm boing' io the sone\n",
            "that i got the mine the sooe that i got the mine the thit then i get it i'm bacl ao you bnt words then i medd to the soae if so this soig it and go the seal gor so do it wotr back boonne wouls the sone that i got the mine the sooe that i got the mine the sooe that i got the mine the thit then i get it i'm bacl ao yhat\n",
            "i'me never ba teere i mever be the reme to feeling like thes i mever ba the realod i wes a fronne woudd i was like that she sole the shit that i got the mine the sooe that i got the mene i don't know what i mever ba the sone that i got the mene i was aod the sone that i got the mene i don't know what i mever ba the sone that i got the mene i was aod the sone that i got the mene i don't know what i mever ba the sone that i got the mene i was aod the sone that i got the mene i don't know what i mever ba the sone that i got the mene i was aod the sone that i got the mene i don"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf7HhERLWsv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4dc-orLXOTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "18134473-241e-41fd-d1f5-57c09a2ba341"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logic_lyrics.txt\t\t    weights-improvement-21-1.9644.hdf5\n",
            "model_1.7654.h5\t\t\t    weights-improvement-22-1.9537.hdf5\n",
            "weights-improvement-01-2.6115.hdf5  weights-improvement-23-1.9452.hdf5\n",
            "weights-improvement-01-2.7537.hdf5  weights-improvement-24-1.9330.hdf5\n",
            "weights-improvement-01-2.9581.hdf5  weights-improvement-25-1.9190.hdf5\n",
            "weights-improvement-02-2.5391.hdf5  weights-improvement-26-1.9107.hdf5\n",
            "weights-improvement-02-2.7551.hdf5  weights-improvement-27-1.8999.hdf5\n",
            "weights-improvement-03-2.4740.hdf5  weights-improvement-28-1.8901.hdf5\n",
            "weights-improvement-04-2.4183.hdf5  weights-improvement-29-1.8829.hdf5\n",
            "weights-improvement-05-2.3681.hdf5  weights-improvement-30-1.8741.hdf5\n",
            "weights-improvement-06-2.3232.hdf5  weights-improvement-31-1.8673.hdf5\n",
            "weights-improvement-07-2.2848.hdf5  weights-improvement-33-1.8663.hdf5\n",
            "weights-improvement-08-2.2499.hdf5  weights-improvement-34-1.8565.hdf5\n",
            "weights-improvement-09-2.2176.hdf5  weights-improvement-35-1.8563.hdf5\n",
            "weights-improvement-10-2.1882.hdf5  weights-improvement-36-1.8409.hdf5\n",
            "weights-improvement-11-2.1599.hdf5  weights-improvement-37-1.8290.hdf5\n",
            "weights-improvement-12-2.1348.hdf5  weights-improvement-39-1.8171.hdf5\n",
            "weights-improvement-13-2.1125.hdf5  weights-improvement-40-1.8156.hdf5\n",
            "weights-improvement-14-2.0871.hdf5  weights-improvement-42-1.8066.hdf5\n",
            "weights-improvement-15-2.0674.hdf5  weights-improvement-43-1.8012.hdf5\n",
            "weights-improvement-16-2.0472.hdf5  weights-improvement-46-1.7977.hdf5\n",
            "weights-improvement-17-2.0281.hdf5  weights-improvement-47-1.7891.hdf5\n",
            "weights-improvement-18-2.0123.hdf5  weights-improvement-48-1.7791.hdf5\n",
            "weights-improvement-19-1.9948.hdf5  weights-improvement-51-1.7654.hdf5\n",
            "weights-improvement-20-1.9789.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEJmrN3X19k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CxQljuGXPvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrwUiSFsXeBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40ae3894-2db6-4342-a9b8-2cff66a0bb71"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i3puawpaCg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}